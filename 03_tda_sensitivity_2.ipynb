{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.int = np.int32\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from tmap.tda import mapper, Filter\n",
    "from tmap.tda.cover import Cover\n",
    "from tmap.tda.metric import Metric\n",
    "from tmap.tda.utils import optimize_dbscan_eps\n",
    "\n",
    "from scipy.spatial.distance import squareform,pdist\n",
    "import pandas as pd\n",
    "\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "code_dir=Path.cwd()\n",
    "project_dir=code_dir.parent\n",
    "input_dir=project_dir/\"input\"\n",
    "output_dir=project_dir/\"output/tda_sensitivity_2/\"\n",
    "tmp_dir=project_dir/\"tmp\"\n",
    "\n",
    "output_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(input_dir/\"data/metadata_df.csv\", index_col=0)\n",
    "oral_microbiome_genus = pd.read_csv(input_dir/\"data/microbiome_genus.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load taxa abundance data, sample metadata and precomputed distance matrix\n",
    "X = oral_microbiome_genus\n",
    "metadata = metadata.loc[metadata.index.isin(X.index)][metadata_variables]\n",
    "X = X.loc[X.index.isin(metadata.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_categories = [col.split(\"_\")[0] for col in metadata.columns.tolist()]\n",
    "microbiome_categories = [\"genus\"] * len(X.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform2node_data(graph, data, mode='mean'):\n",
    "    map_fun = {'sum': np.sum,\n",
    "               \"mean\": np.nanmean}\n",
    "    if mode not in [\"sum\", \"mean\"]:\n",
    "        raise SyntaxError('Wrong provided parameters.')\n",
    "    else:\n",
    "        aggregated_fun = map_fun[mode]\n",
    "\n",
    "    nodes = graph.nodes\n",
    "    dv = data.values\n",
    "    if data is not None:\n",
    "        node_data = {nid: aggregated_fun(dv[attr['sample'], :], 0)\n",
    "                     for nid, attr in nodes.items()}\n",
    "        node_data = pd.DataFrame.from_dict(node_data,\n",
    "                                           orient='index',\n",
    "                                           columns=data.columns)\n",
    "        return node_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safepy import safe\n",
    "\n",
    "sensitivity_parameter_dict = {\n",
    "    \"cover_overlap\" : [0.75,0.5,0.99],\n",
    "    \"cover_resolution\" : [10,20,40,50],\n",
    "    \"mapper_eps_threshold\" : [99,90,85],\n",
    "    \"mapper_lens\": [Filter.MDS, Filter.UMAP],\n",
    "    \"safe_distance_thresh\": [0.5,0.99],\n",
    "    \"safe_neighborhood_radius\":[0.05,0.15],\n",
    "}\n",
    "\n",
    "sensitivity_result_dict_er = {}\n",
    "sensitivity_result_dict_cluster = {}\n",
    "\n",
    "for string,sensitivity_parameter_list in sensitivity_parameter_dict.items():\n",
    "    for sensitivity_parameter in sensitivity_parameter_list:\n",
    "        if string == \"cover_resolution\":\n",
    "            resolution = sensitivity_parameter\n",
    "            overlap = 0.75\n",
    "            eps_threshold = 95\n",
    "            mapper_lens = Filter.PCOA\n",
    "            mapper_distance_metric = \"braycurtis\"\n",
    "            safe_distance_thresh = 0.75\n",
    "            safe_neighborhood_radius = 0.1\n",
    "\n",
    "        elif string == \"cover_overlap\":\n",
    "            resolution = 30\n",
    "            overlap = sensitivity_parameter\n",
    "            eps_threshold = 95\n",
    "            mapper_lens = Filter.PCOA\n",
    "            mapper_distance_metric = \"braycurtis\"\n",
    "            safe_distance_thresh = 0.75\n",
    "            safe_neighborhood_radius = 0.1\n",
    "        \n",
    "        elif string == \"mapper_eps_threshold\":\n",
    "            resolution = 30\n",
    "            overlap = 0.75\n",
    "            eps_threshold = sensitivity_parameter\n",
    "            mapper_lens = Filter.PCOA\n",
    "            mapper_distance_metric = \"braycurtis\"\n",
    "            safe_distance_thresh = 0.75\n",
    "            safe_neighborhood_radius = 0.1\n",
    "\n",
    "        elif string == \"mapper_lens\":\n",
    "            resolution = 30\n",
    "            overlap = 0.75\n",
    "            eps_threshold = 95\n",
    "            mapper_lens = sensitivity_parameter\n",
    "            mapper_distance_metric = \"braycurtis\"\n",
    "            safe_distance_thresh = 0.75\n",
    "            safe_neighborhood_radius = 0.1\n",
    "\n",
    "        elif string == \"safe_distance_thresh\":\n",
    "            resolution = 30\n",
    "            overlap = 0.75\n",
    "            eps_threshold = 95\n",
    "            mapper_lens = Filter.PCOA\n",
    "            mapper_distance_metric = \"braycurtis\"\n",
    "            safe_distance_thresh = sensitivity_parameter\n",
    "            safe_neighborhood_radius = 0.1\n",
    "\n",
    "        elif string == \"safe_neighborhood_radius\":\n",
    "            resolution = 30\n",
    "            overlap = 0.75\n",
    "            eps_threshold = 95\n",
    "            mapper_lens = Filter.PCOA\n",
    "            mapper_distance_metric = \"braycurtis\"\n",
    "            safe_distance_thresh = 0.75\n",
    "            safe_neighborhood_radius = sensitivity_parameter\n",
    "\n",
    "        ################\n",
    "        #Mapper\n",
    "        ################\n",
    "\n",
    "        # TDA Step1. initiate a Mapper\n",
    "        tm = mapper.Mapper(verbose=1)\n",
    "\n",
    "        # TDA Step2. Projection\n",
    "        dm = squareform(pdist(X,metric=mapper_distance_metric))\n",
    "        metric = Metric(metric=\"precomputed\")\n",
    "        lens = [mapper_lens(components=[0, 1], metric=metric, random_state=100)]\n",
    "        projected_X = tm.filter(dm, lens=lens)\n",
    "\n",
    "        # Step4. Covering, clustering & mapping\n",
    "        eps = optimize_dbscan_eps(X, threshold=eps_threshold)\n",
    "        clusterer = DBSCAN(eps=eps, min_samples=3)\n",
    "        cover = Cover(projected_data=MinMaxScaler().fit_transform(projected_X), resolution=resolution, overlap=overlap)#resolution=40, overlap=0.75)\n",
    "        graph = tm.map(data=X, cover=cover, clusterer=clusterer)\n",
    "        print(graph.info())\n",
    "\n",
    "        ################\n",
    "        #SAFE\n",
    "        ################\n",
    "\n",
    "        initial_nodepos = {idx:graph.nodePos[idx] for idx in range(graph.nodePos.shape[0])}\n",
    "        pos = nx.spring_layout(graph, k = 0.2, pos = initial_nodepos, seed=42)\n",
    "\n",
    "        graph.nodePos = np.array([pos[key] for key in pos.keys()])\n",
    "\n",
    "        for idx, node in enumerate(graph.nodes):\n",
    "            graph.nodes[idx][\"pos\"] = pos[idx].tolist()\n",
    "\n",
    "        edgelist_3col = nx.to_pandas_edgelist(graph)\n",
    "        edgelist_3col[\"dist\"] = 1\n",
    "        edgelist_3col.to_csv(output_dir/f\"{string}_{str(sensitivity_parameter).replace('.','p')}_mapper_graph_3col.txt\", sep=\"\\t\", index=False, header=None)\n",
    "\n",
    "        metadata_transformed = transform2node_data(graph, metadata, mode=\"mean\")\n",
    "        metadata_plus_imaging_transformed = transform2node_data(graph, metadata_plus_imaging, mode=\"mean\")\n",
    "        oral_microbiome_genus_transformed = transform2node_data(graph, oral_microbiome_genus, mode=\"mean\")\n",
    "        oral_microbiome_phylum_transformed = transform2node_data(graph, oral_microbiome_phylum, mode=\"mean\")\n",
    "        data_transformed = metadata_transformed.join(oral_microbiome_genus_transformed)\n",
    "        data_transformed.to_csv(output_dir/f\"{string}_{str(sensitivity_parameter).replace('.','p')}_mapper_graph_metadata.txt\", sep=\"\\t\", index=True)\n",
    "\n",
    "        sf = safe.SAFE(path_to_safe_data=f\"{output_dir}/safe_{string}_{str(sensitivity_parameter).replace('.','p')}/\")\n",
    "        sf.random_seed = 0\n",
    "        sf.attribute_distance_threshold = safe_distance_thresh\n",
    "        sf.neighborhood_radius = safe_neighborhood_radius\n",
    "        sf.load_network(network_file=f\"{output_dir}/{string}_{str(sensitivity_parameter).replace('.','p')}_mapper_graph_3col.txt\")\n",
    "        sf.load_attributes(attribute_file=f\"{output_dir}/{string}_{str(sensitivity_parameter).replace('.','p')}_mapper_graph_metadata.txt\")\n",
    "        sf.define_neighborhoods()\n",
    "\n",
    "        num_permutations = 5000\n",
    "        sf.compute_pvalues(num_permutations=num_permutations)\n",
    "\n",
    "        network_enrichment_scores = pd.DataFrame(sf.nes, columns=data_transformed.columns)\n",
    "        network_enrichment_scores_signif = pd.DataFrame(sf.nes_binary, columns=data_transformed.columns)\n",
    "        network_enrichment_scores_signif_pos = (network_enrichment_scores > 0) & (network_enrichment_scores_signif)\n",
    "        network_enrichment_scores_signif_neg = (network_enrichment_scores < 0) & (network_enrichment_scores_signif)\n",
    "\n",
    "        safe_summary = sf.attributes.copy()\n",
    "        safe_summary.drop(\"id\", axis=1)\n",
    "        safe_summary.set_index(\"name\", inplace=True)\n",
    "\n",
    "        safe_summary[f\"{string}_{sensitivity_parameter}_enrichment_ratio\"] = safe_summary[\"num_neighborhoods_enriched\"] / len(graph.nodes)\n",
    "        sensitivity_result_dict_er[f\"{string}_{sensitivity_parameter}_enrichment_ratio\"] = safe_summary[f\"{string}_{sensitivity_parameter}_enrichment_ratio\"]\n",
    "\n",
    "        ################\n",
    "        #Clustering\n",
    "        ################\n",
    "\n",
    "        from sklearn.cluster import KMeans\n",
    "        import numpy as np\n",
    "\n",
    "        positions = pd.DataFrame(nx.get_node_attributes(graph, \"pos\")).T\n",
    "        positions.columns = [\"0\", \"1\"]\n",
    "\n",
    "        clustering_input = positions.copy()\n",
    "\n",
    "        clustering_input.columns = [str(idx) for idx in list(range(clustering_input.shape[1]))]\n",
    "        n_clusters = 2\n",
    "\n",
    "        clustering = KMeans(n_clusters=2, random_state=42).fit(clustering_input)\n",
    "        positions[\"cluster\"] = clustering.labels_\n",
    "\n",
    "        import itertools\n",
    "\n",
    "        node_subject_mapping_idx_dict = {node:list(graph.nodes[idx][\"sample\"]) for idx,node in enumerate(graph.nodes)}\n",
    "        node_subject_mapping_dict = {node:list(graph.nodes[idx][\"sample_names\"]) for idx,node in enumerate(graph.nodes)}\n",
    "\n",
    "        all_subject_indices = sorted(set(itertools.chain(*node_subject_mapping_dict.values())))\n",
    "        node_subject_df = pd.DataFrame(0, index = all_subject_indices, columns = list(graph.nodes))\n",
    "\n",
    "        for node, subjects in node_subject_mapping_dict.items():\n",
    "            for subject in subjects:\n",
    "                node_subject_df.loc[subject, node] = 1\n",
    "\n",
    "        node_subject_df = node_subject_df.loc[metadata.index[metadata.index.isin(node_subject_df.index)]]\n",
    "\n",
    "        subject_group_df = node_subject_df.T.join(positions[\"cluster\"]).groupby(\"cluster\").sum().T\n",
    "\n",
    "        def determine_cluster(row):\n",
    "            if row[0] > 0 and row[1] > 0:\n",
    "                return -1\n",
    "            elif row[0] > 0:\n",
    "                return 0\n",
    "            elif row[1] > 0:\n",
    "                return 1\n",
    "            else:\n",
    "                return np.nan\n",
    "            \n",
    "        subject_group_df[\"cluster\"] = subject_group_df.apply(determine_cluster, axis=1)\n",
    "\n",
    "        sensitivity_result_dict_cluster[f\"{string}_{sensitivity_parameter}_clustering\"] = subject_group_df[\"cluster\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity_result_df_er = pd.DataFrame(sensitivity_result_dict_er)\n",
    "sensitivity_result_df_er.columns = [col.replace(\".\",\"p\") for col in sensitivity_result_df_er.columns]\n",
    "sensitivity_result_df_cluster = pd.DataFrame(sensitivity_result_dict_cluster)\n",
    "sensitivity_result_df_cluster.columns = [col.replace(\".\",\"p\") for col in sensitivity_result_df_cluster.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_enrichment_ratio = pd.read_csv(project_dir/\"output/tda/metadata_safe_summary.csv\", index_col=0)\n",
    "original_clustering =  pd.read_csv(project_dir/\"output/tda/cluster_analysis/subject_clustering.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_robustness_df = original_clustering.copy()\n",
    "for i in sensitivity_result_df_cluster.keys():\n",
    "    cluster_robustness_df = cluster_robustness_df.join(sensitivity_result_df_cluster[i], rsuffix=f\"_{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "spearman_er_dict = {}\n",
    "ari_clustering_dict = {}\n",
    "\n",
    "for i in sensitivity_result_df_er.keys():\n",
    "    spearman_er_dict[f\"sensitivity_{i}_spearman\"] = spearmanr(original_enrichment_ratio[\"enrichment_ratio\"], sensitivity_result_df_er[i])[0]\n",
    "\n",
    "\n",
    "for i in sensitivity_result_df_cluster.keys():\n",
    "    cluster_robustness_i_df = cluster_robustness_df[[\"cluster\",f\"{i}\"]].copy()\n",
    "    cluster_robustness_i_df.dropna(inplace=True)\n",
    "    ari_clustering_dict[f\"sensitivity_{i}_ari\"] = adjusted_rand_score(cluster_robustness_i_df[\"cluster\"], cluster_robustness_i_df[f\"{i}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = pd.DataFrame(index = sensitivity_result_df_er.columns, columns = [\"Enrichment ratio\", \"Clustering\"])\n",
    "for string in sensitivity_parameter_dict.keys():\n",
    "    corr_df.loc[f\"{string}\", \"Enrichment ratio\"] = sensitivity_result_df_er[f\"{string}_enrichment_ratio\"]\n",
    "    sensitivity_result_df_cluster[f\"{string}\"] = sensitivity_result_df_cluster[f\"{string}_{sensitivity_parameter_list[0]}_clustering\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainstat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
